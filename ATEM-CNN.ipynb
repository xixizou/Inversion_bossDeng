{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGES LOADED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "print (\"PACKAGES LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.loadtxt('data\\Input_Altitude.txt')\n",
    "A2 = np.loadtxt('data\\Output_Altitude.txt')\n",
    "A3 = np.loadtxt('data\\Altitude.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100)\n",
      "(40000, 300)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "print(A1.shape)\n",
    "print(A2.shape)\n",
    "print(A3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_in = np.mean(A1, axis=0)\n",
    "std_in = np.std(A1, axis=0)\n",
    "B1 = (A1 - mean_in) / std_in\n",
    "B2 = A2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 100)\n",
      "(36000, 300)\n",
      "(4000, 100)\n",
      "(4000, 300)\n",
      "(36000,)\n",
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test, in_train, in_test, out_train, out_test, H_train, H_test = train_test_split(\n",
    "   B1, B2, A1, A2, A3, test_size=0.10, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(H_train.shape)\n",
    "print(H_test.shape)\n",
    "num_examples=X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('network_save\\mean_ATEM.txt', mean_in)\n",
    "np.savetxt('network_save\\std_ATEM.txt', std_in)\n",
    "print('Ready!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48. 59. 58. ... 73. 70. 41.]\n"
     ]
    }
   ],
   "source": [
    "print(H_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data\\input_altitude_test.txt', in_test)\n",
    "np.savetxt('data\\output_altitude_test.txt', out_test)\n",
    "np.savetxt('data\\height_test.txt',H_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a \n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    a = tf.reduce_sum(tf.multiply(vector_a, vector_b), axis=1)\n",
    "    b = tf.multiply(tf.norm(vector_a, axis=1), tf.norm(vector_b, axis=1))\n",
    "    sim = tf.reduce_mean(tf.divide(a, b))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现Batch Normalization\n",
    "def bn1_layer(x,is_training,name='BatchNorm',moving_decay=0.9,eps=1e-5):\n",
    "    # 获取输入维度并判断是否匹配卷积层(4)或者全连接层(2)\n",
    "    shape = x.shape\n",
    "    assert len(shape) in [2,4]\n",
    "\n",
    "    param_shape = shape[-1]\n",
    "    with tf.variable_scope(name_or_scope='', reuse = tf.AUTO_REUSE):\n",
    "#     with tf.variable_scope(name)：\n",
    "        # 声明BN中唯一需要学习的两个参数，y=gamma*x+beta\n",
    "        gamma = tf.get_variable('gamma1',param_shape,initializer=tf.constant_initializer(1))\n",
    "        beta  = tf.get_variable('beat1', param_shape,initializer=tf.constant_initializer(0))\n",
    "\n",
    "        # 计算当前整个batch的均值与方差\n",
    "        axes = list(range(len(shape)-1))\n",
    "        batch_mean, batch_var = tf.nn.moments(x,axes,name='moments')\n",
    "\n",
    "        # 采用滑动平均更新均值与方差\n",
    "        ema = tf.train.ExponentialMovingAverage(moving_decay)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean,batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        # 训练时，更新均值与方差，测试时使用之前最后一次保存的均值与方差\n",
    "        mean, var = tf.cond(tf.equal(is_training,True),mean_var_with_update,\n",
    "                lambda:(ema.average(batch_mean),ema.average(batch_var)))\n",
    "\n",
    "        # 最后执行batch normalization\n",
    "        return tf.nn.batch_normalization(x,mean,var,beta,gamma,eps)\n",
    "    \n",
    "    # 实现Batch Normalization\n",
    "def bn2_layer(x,is_training,name='BatchNorm',moving_decay=0.9,eps=1e-5):\n",
    "    # 获取输入维度并判断是否匹配卷积层(4)或者全连接层(2)\n",
    "    shape = x.shape\n",
    "    assert len(shape) in [2,4]\n",
    "\n",
    "    param_shape = shape[-1]\n",
    "    with tf.variable_scope(name_or_scope='', reuse = tf.AUTO_REUSE):\n",
    "#     with tf.variable_scope(name)：\n",
    "        # 声明BN中唯一需要学习的两个参数，y=gamma*x+beta\n",
    "        gamma = tf.get_variable('gamma2',param_shape,initializer=tf.constant_initializer(1))\n",
    "        beta  = tf.get_variable('beat2', param_shape,initializer=tf.constant_initializer(0))\n",
    "\n",
    "        # 计算当前整个batch的均值与方差\n",
    "        axes = list(range(len(shape)-1))\n",
    "        batch_mean, batch_var = tf.nn.moments(x,axes,name='moments')\n",
    "\n",
    "        # 采用滑动平均更新均值与方差\n",
    "        ema = tf.train.ExponentialMovingAverage(moving_decay)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean,batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        # 训练时，更新均值与方差，测试时使用之前最后一次保存的均值与方差\n",
    "        mean, var = tf.cond(tf.equal(is_training,True),mean_var_with_update,\n",
    "                lambda:(ema.average(batch_mean),ema.average(batch_var)))\n",
    "\n",
    "        # 最后执行batch normalization\n",
    "        return tf.nn.batch_normalization(x,mean,var,beta,gamma,eps)\n",
    "    \n",
    "def bn3_layer(x,is_training,name='BatchNorm',moving_decay=0.9,eps=1e-5):\n",
    "    # 获取输入维度并判断是否匹配卷积层(4)或者全连接层(2)\n",
    "    shape = x.shape\n",
    "    assert len(shape) in [2,4]\n",
    "\n",
    "    param_shape = shape[-1]\n",
    "    with tf.variable_scope(name_or_scope='', reuse = tf.AUTO_REUSE):\n",
    "#     with tf.variable_scope(name)：\n",
    "        # 声明BN中唯一需要学习的两个参数，y=gamma*x+beta\n",
    "        gamma = tf.get_variable('gamma3',param_shape,initializer=tf.constant_initializer(1))\n",
    "        beta  = tf.get_variable('beat3', param_shape,initializer=tf.constant_initializer(0))\n",
    "\n",
    "        # 计算当前整个batch的均值与方差\n",
    "        axes = list(range(len(shape)-1))\n",
    "        batch_mean, batch_var = tf.nn.moments(x,axes,name='moments')\n",
    "\n",
    "        # 采用滑动平均更新均值与方差\n",
    "        ema = tf.train.ExponentialMovingAverage(moving_decay)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean,batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        # 训练时，更新均值与方差，测试时使用之前最后一次保存的均值与方差\n",
    "        mean, var = tf.cond(tf.equal(is_training,True),mean_var_with_update,\n",
    "                lambda:(ema.average(batch_mean),ema.average(batch_var)))\n",
    "\n",
    "        # 最后执行batch normalization\n",
    "        return tf.nn.batch_normalization(x,mean,var,beta,gamma,eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 200)\n",
      "(36000, 200)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "training_epochs = 1000\n",
    "H_200_train = np.tile(H_train,(200,1))\n",
    "H_200_train = np.transpose(H_200_train)\n",
    "H_200_test = np.tile(H_test,(200,1))\n",
    "H_200_test = np.transpose(H_200_test)\n",
    "print(H_200_test.shape)\n",
    "print(H_200_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_queue = tf.train.slice_input_producer([X_train, Y_train,H_200_train], shuffle=False)\n",
    "xtrain_batch, ytrain_batch, htrain_batch = tf.train.shuffle_batch(input_queue,\n",
    "                                                   batch_size=128, capacity=500,\n",
    "                                                   min_after_dequeue=200, num_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 100])\n",
    "y = tf.placeholder(tf.float32, [None, 300])\n",
    "h = tf.placeholder(tf.float32, [None, 200])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "ornot = tf.placeholder(tf.float32)\n",
    "x_image = tf.reshape(x, [-1, 1, 100, 1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(shape):\n",
    "    #给权重制造一些随机的噪声来打破完全对称，比如截断的正态分布噪声，标准差设为0.1 \n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    return w\n",
    "\n",
    "def biases(shape):\n",
    "    #给偏置增加一些小的正值(0.1)，来避免死亡节点(dead neurons) \n",
    "    b = tf.Variable(tf.constant(0.1,shape=shape))\n",
    "    return b\n",
    "\n",
    "def conv2d(x,W): \n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')  \n",
    "\n",
    "def max_pool_2x2(x): \n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')   #池化尺寸[1,3,3,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1 Layer  \n",
    "W_conv1 = weights([1, 15, 1, 32])  \n",
    "b_conv1 = biases([32])  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "# h_pool1 = bn2_layer(h_pool1,ornot)\n",
    "\n",
    "# Conv2 Layer  \n",
    "W_conv2 = weights([1, 15, 32, 64])  \n",
    "b_conv2 = biases([64])  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  \n",
    "# h_pool2 = bn3_layer(h_pool2,ornot)\n",
    "\n",
    "# Conv2 Layer  \n",
    "W_conv3 = weights([1, 5, 64, 128])  \n",
    "b_conv3 = biases([128])  \n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)  \n",
    "h_pool3 = max_pool_2x2(h_conv3)  \n",
    "# h_pool3 = bn4_layer(h_pool3,ornot)\n",
    "\n",
    "#full connection  \n",
    "W_fc1 = weights([13*128, 800])  \n",
    "b_fc1 = biases([800])  \n",
    "h_pool2_flat = tf.reshape(h_pool3, [-1, 13*128]) \n",
    "L1 = tf.add(tf.matmul(h_pool2_flat, W_fc1), b_fc1)\n",
    "L1_bn = bn1_layer(L1,ornot)\n",
    "h_fc1 = tf.nn.relu(L1_bn)\n",
    "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)   \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "\n",
    "h_fc1_stack = tf.concat([h_fc1_drop, h],1)\n",
    "\n",
    "W_fc2 = weights([1000, 600])  \n",
    "b_fc2 = biases([600])  \n",
    "L2 = tf.add(tf.matmul(h_fc1_stack, W_fc2), b_fc2)\n",
    "L2_bn = bn2_layer(L2,ornot)\n",
    "h_fc2 = tf.nn.relu(L2_bn)\n",
    "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)   \n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "w3 = tf.Variable(tf.truncated_normal([600, 300], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([300]) + 0.1)\n",
    "prediction = tf.matmul(h_fc2_drop,w3 ) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_reg = 0.001\n",
    "regularizer = tf.contrib.layers.l2_regularizer(L2_reg)\n",
    "reg_loss =regularizer(W_fc1) +  regularizer(w3)\n",
    "#定义损失函数、优化器和准确率。\n",
    "loss_sqr = tf.reduce_mean(tf.square(prediction - y))\n",
    "loss_abs = tf.reduce_mean(tf.abs(prediction - y)) \n",
    "loss_cos= 1 - cos_sim(prediction , y)\n",
    "#\n",
    "loss = loss_sqr + reg_loss\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss: 0.98342454 rmse_train: 0.40904972  rmse_test: 0.44670776   mae_test: 0.51202846  Cos: 0.031767607\n",
      "save to path network_save\\linear_cnn.ckpt\n",
      "Running time: 42.511943 Seconds\n"
     ]
    }
   ],
   "source": [
    "start =time.clock() \n",
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    step = 0\n",
    "    total_batch =  int(num_examples/batch_size)\n",
    "    for epoch in range (training_epochs):\n",
    "        for i in range (total_batch):\n",
    "            batch_x, batch_y, batch_h=sess.run([xtrain_batch, ytrain_batch, htrain_batch])\n",
    "            sess.run(train_step, feed_dict ={x:batch_x, y:batch_y, keep_prob:0.7,ornot:True, h:batch_h})\n",
    "        if epoch % 1 ==0:\n",
    "            loss_train = sess.run(loss, feed_dict ={x:batch_x, y:batch_y, keep_prob:1,ornot:False, h:batch_h})\n",
    "            rmse_train = sess.run(loss_sqr, feed_dict ={x:batch_x, y:batch_y, keep_prob:1,ornot:False, h:batch_h})\n",
    "            test_data = X_test\n",
    "            test_label = Y_test\n",
    "            rmse_test = sess.run(loss_sqr, feed_dict ={x:test_data, y:test_label, keep_prob:1.0,ornot:False, h:H_200_test})\n",
    "            mae_test = sess.run(loss_abs, feed_dict ={x:test_data, y:test_label, keep_prob:1.0,ornot:False, h:H_200_test} )\n",
    "            cos_test = sess.run(loss_cos, feed_dict ={x:test_data, y:test_label, keep_prob:1.0,ornot:False, h:H_200_test} )\n",
    "            \n",
    "            print(\"step \" + str(epoch) + \": loss: \" + str(loss_train )+ \" rmse_train: \" + str( rmse_train )+ \\\n",
    "                \"  rmse_test: \" + str( rmse_test) +  \"   mae_test: \" + str(mae_test) +\"  Cos: \" + str(cos_test))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)  \n",
    "    \n",
    "    save_path = saver.save(sess,\"network_save\\cnn.ckpt\")\n",
    "    print(\"save to path\",save_path)\n",
    "\n",
    "    end = time.clock()\n",
    "    print('Running time: %s Seconds'%(end-start))\n",
    "    \n",
    "    pre_test = sess.run(prediction, feed_dict ={x:test_data, keep_prob:1.0,ornot:False, h:H_200_test})\n",
    "    np.savetxt('data\\output_altitude_cnn.txt', pre_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
